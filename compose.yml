version: '3.9'
services:
  resource-svc-db: &postgres
    image: postgres:16.3-alpine3.20
    container_name: resource-svc-db
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5471:5432"
    volumes:
      - ./config/initdb/resource-svc:/docker-entrypoint-initdb.d
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "postgres" ]
      interval: 10s
      timeout: 10s
      retries: 20
    networks:
      - internal

  song-svc-db:
    <<: *postgres
    container_name: song-svc-db
    ports:
      - "5472:5432"
    volumes:
      - ./config/initdb/song-svc:/docker-entrypoint-initdb.d

  storage-svc-db:
    <<: *postgres
    container_name: storage-svc-db
    ports:
      - "5474:5432"
    volumes:
      - ./config/initdb/storage-svc:/docker-entrypoint-initdb.d

  localstack:
    image: localstack/localstack
    container_name: localstack
    ports:
      - "4566:4566"  # LocalStack port exposed for S3 interactions
    environment:
      - SERVICES=s3
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
    volumes:
      - ./config/localstack-volume:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: [ "CMD-SHELL", "awslocal s3 ls || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 20
    networks:
      - internal

  activemq:
    image: apache/activemq-classic:6.1.1
    container_name: activemq
    ports:
      - "8161:8161"  # ActiveMQ web console port exposed
      - "61616:61616"  # ActiveMQ messaging port exposed
    environment:
      ACTIVEMQ_REMOVE_DEFAULT_ACCOUNTS: "true"
      ACTIVEMQ_ADMIN_LOGIN: ${ACTIVEMQ_ADMIN_LOGIN}
      ACTIVEMQ_ADMIN_PASSWORD: ${ACTIVEMQ_ADMIN_PASSWORD}
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:8161 | grep -q 'ActiveMQ'" ]
      interval: 10s
      timeout: 10s
      retries: 20
    networks:
      - internal

  eureka-server:
    build: ./eureka-svc
    image: mf-eureka-svc
    container_name: eureka-server
    environment:
      HOSTNAME: ${EUREKA_HOST}
      LOGSTASH_HOST: ${LOGSTASH_HOST}
      LOGSTASH_PORT: ${LOGSTASH_PORT}
    ports:
      - "8761:8761"  # Eureka server port exposed; needs to be accessible to all the services
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://eureka-server:8761/eureka-svc/health" ]
      interval: 10s
      timeout: 10s
      retries: 50
    networks:
      - internal

  gateway-svc:
    build: ./gateway-svc
    image: mf-gateway-svc
    container_name: gateway-svc
    environment:
      EUREKA_HOST: ${EUREKA_HOST}
      LOGSTASH_HOST: ${LOGSTASH_HOST}
      LOGSTASH_PORT: ${LOGSTASH_PORT}
    ports:
      - "8070:8070"  # Gateway port exposed for external access
    depends_on:
      eureka-server:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://gateway-svc:8070/gateway-svc/health" ]
      interval: 10s
      timeout: 10s
      retries: 50
    networks:
      - internal
      - external

  song-svc:
    build: ./song-svc
    image: mf-song-svc
    container_name: song-svc
    environment:
      DB_HOST: song-svc-db
      DB_PORT: ${DB_PORT}
      DB_USERNAME: ${DB_USERNAME}
      DB_PASSWORD: ${DB_PASSWORD}
      EUREKA_HOST: ${EUREKA_HOST}
      LOGSTASH_HOST: ${LOGSTASH_HOST}
      LOGSTASH_PORT: ${LOGSTASH_PORT}
    #    ports:
    #      - "8072:8072"
    depends_on:
      song-svc-db:
        condition: service_healthy
      eureka-server:
        condition: service_healthy
      logstash:
        condition: service_started
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://song-svc:8072/song-svc/health" ]
      interval: 10s
      timeout: 10s
      retries: 50
    networks:
      - internal

  storage-svc:
    build: ./storage-svc
    image: mf-storage-svc
    container_name: storage-svc
    environment:
      DB_HOST: storage-svc-db
      DB_PORT: ${DB_PORT}
      DB_USERNAME: ${DB_USERNAME}
      DB_PASSWORD: ${DB_PASSWORD}
      EUREKA_HOST: ${EUREKA_HOST}
      LOGSTASH_HOST: ${LOGSTASH_HOST}
      LOGSTASH_PORT: ${LOGSTASH_PORT}
    #    ports:
    #      - "8074:8074"
    depends_on:
      storage-svc-db:
        condition: service_healthy
      eureka-server:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://storage-svc:8074/storage-svc/health" ]
      interval: 10s
      timeout: 10s
      retries: 50
    networks:
      - internal

  resource-svc:
    build: ./resource-svc
    image: mf-resource-svc
    container_name: resource-svc
    environment:
      DB_HOST: resource-svc-db
      DB_PORT: ${DB_PORT}
      DB_USERNAME: ${DB_USERNAME}
      DB_PASSWORD: ${DB_PASSWORD}
      EUREKA_HOST: ${EUREKA_HOST}
      AWS_ENDPOINT_URL: ${AWS_ENDPOINT_URL}
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY}
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
      ACTIVEMQ_HOST: ${ACTIVEMQ_HOST}
      ACTIVEMQ_USERNAME: ${ACTIVEMQ_USERNAME}
      ACTIVEMQ_PASSWORD: ${ACTIVEMQ_PASSWORD}
      LOGSTASH_HOST: ${LOGSTASH_HOST}
      LOGSTASH_PORT: ${LOGSTASH_PORT}
    #    ports:
    #      - "8071:8071"
    depends_on:
      resource-svc-db:
        condition: service_healthy
      song-svc:
        condition: service_healthy
      storage-svc:
        condition: service_healthy
      eureka-server:
        condition: service_healthy
      localstack:
        condition: service_healthy
      activemq:
        condition: service_started
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://resource-svc:8071/resource-svc/health" ]
      interval: 10s
      timeout: 10s
      retries: 50
    networks:
      - internal

  resource-proc:
    build: ./resource-proc
    image: mf-resource-proc
    container_name: resource-proc
    environment:
      EUREKA_HOST: ${EUREKA_HOST}
      ACTIVEMQ_HOST: ${ACTIVEMQ_HOST}
      ACTIVEMQ_USERNAME: ${ACTIVEMQ_USERNAME}
      ACTIVEMQ_PASSWORD: ${ACTIVEMQ_PASSWORD}
      LOGSTASH_HOST: ${LOGSTASH_HOST}
      LOGSTASH_PORT: ${LOGSTASH_PORT}
    #    ports:
    #      - "8073:8073"
    depends_on:
      eureka-server:
        condition: service_healthy
      activemq:
        condition: service_started
      resource-svc:
        condition: service_healthy
      song-svc:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://resource-proc:8073/resource-proc/health" ]
      interval: 10s
      timeout: 10s
      retries: 50
    networks:
      - internal

  prometheus:
    image: prom/prometheus:v2.51.1
    container_name: prometheus
    volumes:
      - ./config/prometheus:/etc/prometheus
      - ./config/prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    networks:
      - internal

  grafana:
    image: grafana/grafana:11.0.0
    container_name: grafana
    volumes:
      - ./config/grafana/grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./config/grafana/dashboards:/etc/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks:
      - internal
      - external

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.16
    container_name: elasticsearch
    environment:
      - node.name=elasticsearch
      - cluster.name=es-docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - cluster.routing.allocation.disk.threshold_enabled=true
      - cluster.routing.allocation.disk.watermark.low=85%
      - cluster.routing.allocation.disk.watermark.high=90%
      - cluster.routing.allocation.disk.watermark.flood_stage=95%
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ./config/elasticsearch/data:/usr/share/elasticsearch/data
    #      - ./config/elasticsearch/config:/usr/share/elasticsearch/config
    ports:
      - "9200:9200"
    healthcheck:
      test: [ "CMD-SHELL", "curl -s http://localhost:9200 >/dev/null || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 20
    networks:
      - internal
  
  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.16
    container_name: logstash
    volumes:
      - ./config/logstash/pipeline:/usr/share/logstash/pipeline
    ports:
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9600" ]
      interval: 10s
      timeout: 10s
      retries: 20
    networks:
      - internal

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.16
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_BASEPATH="/kibana"
      - SERVER_REWRITEBASEPATH="true"
    volumes:
      - ./config/kibana/data:/usr/share/kibana/data
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - internal
      - external
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5601/kibana/api/status" ]
      interval: 30s
      timeout: 10s
      retries: 5


networks:
  internal:
  external:

# Prometheus:
#  http://localhost:8070/prometheus, http://localhost:9090
#  http://localhost:9090/targets
# Grafana:
# http://localhost:8070/grafana, http://localhost:3000
# http://localhost:8070/grafana/d/microservices/microservices-dashboard?orgId=1
# Kibana:
# http://localhost:8070/kibana, http://localhost:5601/kibana


